# Introduction

- Foundation Models (FMs) are scalable sequence models like Transformers but face challenges with long sequences due to quadratic complexity.
- Structured State Space Models (SSMs) blend RNNs and CNNs, offering efficient linear scaling and effectively capturing long-range dependencies.
- Selective SSMs introduce input-based selection mechanisms, enhancing efficiency and relevance of data processing. The Mamba architecture simplifies deep sequence models, combining SSMs with Transformer MLP blocks for improved performance on dense data modalities.

---

# State Space Model (SSM)

## Definition of SSM (Continuous Term)

![Untitled](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/c32e428b-b5c0-41dc-838b-224e2d382d1a)

### **Learnable Matrices: A, B, C, D**

- **A (State Matrix):** Controls how the current state affects the next state. It represents the internal dynamics of the system.
- **B (Control Matrix):** Determines how the input variables affect the state of the system. It links external controls or inputs to changes in the system's state.
- **C (Output Matrix):** Defines how the state variables are transformed into output variables. It captures how the internal state is observed or measured.
- **D (Command Matrix):** Directly relates input variables to output variables, often representing a direct feedthrough or immediate effect of inputs on outputs.

### **System Equations**

- **State Equation:** **x'(t) = Ax(t) + Bu(t)** represents how the state of the system evolves over time, where **x'(t)** denotes the time derivative of **x(t)**, indicating how the state changes.
- **Output Equation:** **y(t) = Cx(t) + Du(t)** links the current state and input to the observable outputs.

## Discretization

<img width="726" alt="Untitled 1" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5985edf4-9217-4969-8e4d-57be243cb8b6">

Discretizing a continuous-time SSM involves converting the state and output equations into forms that update the system state and output at discrete time intervals. The time is discretized into steps of a fixed size Δ*t*, leading to discrete time instants *t*0,*t*1,*t*2,…,*tn*.

1. **State Discretization:** The state equation needs to be reformulated so that we can compute the state *x*[*k*+1] at the next time step *k*+1 from the current state *x*[*k*] and current input *u*[*k*] using discrete approximations.
2. **Output Discretization:** Similarly, the output equation needs to be adapted to compute outputs *y*[*k*] based on the discrete states and inputs.

[Introduction to State Space Models (SSM)](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)

---

# Selective State Space Model (Selective SSM)

## Reasons of decreasing ability of SSM

### Selective Copying

- **Selective Observability:** Unlike traditional SSMs where all state variables are considered observable, selective SSMs focus on scenarios where only a subset of the state variables can be observed or are relevant to the task at hand.
- **Selective Control:** In some systems, inputs might influence only specific parts of the state vector, a feature that selective SSMs can model more naturally than their non-selective counterparts.

![Untitled 2](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/17a08a55-9206-4837-9806-8e775bb9817d)

*(recursive/convolutional) SSMs underperform in this task due to **linear time invariance**. The matrices A, B, C remain constant for all tokens generated by the SSM.*

*Consequently, SSMs are incapable of performing content-aware inference. This is because the fixed A, B, C matrices treat each token equally, posing a problem for situations where we desire the SSM to reason about the input (prompt).*

### Induction Heads

![Untitled 3](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2ec5b4b1-38cd-4b99-9c6f-d657cdb6db0c)

![Untitled 4](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/5972c0a0-cfae-4d10-9071-9f933b24fc60)

## Selective SSM

- Mamba dynamically parameterizes SSM parameters based on inputs, enhancing adaptability and accuracy by focusing on relevant data and filtering out noise.

![Untitled 5](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/f18707d7-01b4-4b32-bd47-51ecaf75a4f9)

![Untitled 6](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/eb14d5f5-3da9-43ca-8763-1e8fd217dc56)

![Untitled 7](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/b8542616-1ae2-46fc-85c2-bc1d5bfd5756)

---

# Mamba?

The incorporation of the Selection Mechanism leads to the loss of the convolutional structure. However, considerable effort is directed towards optimizing Mamba for efficiency on contemporary GPU hardware, utilizing hardware optimization strategies akin to those in Tri Dao’s Flash Attention. Thanks to these optimizations, Mamba can operate more swiftly than Transformers of similar size.

![Untitled 8](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/8793858c-8f89-4492-b480-e6cce6e58aec)

![Untitled 9](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ac78020a-a93b-4c98-9eee-a77da09ff200)

Selective State Space Models (SSMs) possess the following characteristics:

- *Recursive SSMs* created through discretization,
- Matrix A initialized with *HiPPO* to capture *long-range dependencies*,
    - HiPPO enables the representation of signals as combinations of polynomial functions, which efficiently captures long-range dependencies and dynamics within data.
    - Despite its potential for enhanced accuracy and efficiency in sequence modeling and time-series analysis, the mathematical complexity of HiPPO and the challenges in implementation necessitate a deep understanding of advanced mathematics and specialized numerical methods.

![Untitled 10](https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2d2cd77e-ddac-4edc-804f-07617193a715)

- A *selective scan algorithm* for selectively compressing information,
- *Hardware-aware algorithms* to increase computational speed.

---

# Conclusion

- A selection mechanism added to structured state space models enables context-sensitive reasoning with linear scalability in sequence length.
- Mamba, an attention-free model incorporating this mechanism, delivers top-tier performance across various fields, rivaling or surpassing strong Transformer models. Mamba's success positions it as a potent general sequence model backbone.
- Selective state space models show promise for foundational models in domains needing extensive context, like genomics, audio, and video.

# Week 4. DDPM

작성자: Junhyeong Park

### 참고 자료
[https://www.youtube.com/watch?v=_JQSMhqXw-4](https://www.youtube.com/watch?v=_JQSMhqXw-4)

<img width="705" alt="Untitled" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/18f549db-170a-4849-a356-3d69bf895a8d">

# Introduction

A diffusion model is trained using variational inference to generate data through a parameterized Markov chain. The diffusion model consists of two main processes: the forward diffusion process, which incrementally adds noise to the data until it assumes the form of a normal distribution, and the reverse process, where this transformation is learned in reverse.

- **Variational inference**: The process of approximating the posterior probability distribution p(z|x) with a more manageable probability distribution q(z).
- **Parameterize**: The act of re-expressing an expression using different parameters. In this process, the number of parameters is usually chosen to be fewer than the order of the expression (e.g., using 2 parameters for a cubic expression), thus creating a mapping function to a lower dimension (e.g., from 3D to 2D).
- **Markov chain**: A stochastic process in which the transition from one state to another depends only on the immediate previous state.

# Background

<img width="784" alt="Untitled 1" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/6052836c-515b-4b0e-bc49-128841c1c398">

### Forward(diffusion) process

<img width="859" alt="Untitled 2" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/a9dc25eb-dd31-4955-b221-5145b1df1c81">

Process that adds the noise from gaussian distribution

<img width="633" alt="Untitled 3" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/89d5066f-7ae9-44ac-bea2-22c0e408bbeb">

- **Gradual Transformation:** This process incrementally adds noise to data samples, progressively transforming them from their original distribution to pure noise.
- **Complexity Reduction:** As noise is added step by step, the complexity of the data is reduced, ultimately leading to a distribution resembling normal noise.
- **Markov Chain Dependency:** The process follows a Markov chain, where each step depends solely on the immediate previous state, ensuring a controlled addition of noise.

### Reverse Process

<img width="804" alt="Untitled 4" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/b3de2b8d-a2c3-41d1-b1f0-544b6524ef45">

- **Noise Removal:** The reverse process starts with pure noise and gradually removes this noise, restoring the data towards its original form.
- **Core of Generation:** This is the central mechanism for generating data, utilizing denoising techniques to recover original data from the noise-added versions.
- **Stepwise Learning:** At each stage, the model learns to produce data with less noise than the previous step, aiming to recreate the original dataset accurately.

### Loss Function

<img width="869" alt="Untitled 5" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/038b23fe-33db-44ca-bb10-a17322c53b1b">

<img width="844" alt="Untitled 6" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/61c32287-b211-43ff-88ef-b9639b24aefe">

The role of the ELBO (Evidence Lower BOund) is used in the process of approximating the observed, but difficult to manage, probability distribution *P*(*z*∣*x*) with a more tractable distribution *Q*(*x*). It aims to minimize the difference (KL Divergence) between these two distributions (*P*(*z*∣*x*) and *Q*(*x*)) during this approximation process.

- **Minimizing Differences:** The objective function aims to minimize the differences between the samples generated by the reverse process and the original data samples.
- **Techniques Used:** It employs methods like Variational Lower Bound (ELBO) or Maximum Likelihood Estimation (MLE) to optimize this difference.
- **KL Divergence Reduction:** Part of the objective can also involve minimizing the KL Divergence between the forward and reverse processes to reduce discrepancies between the two distributions.

# Diffusion models and denoising encoders

### Objective Function

<img width="737" alt="Untitled 7" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/61d0d6ec-bd4a-41fb-9c6d-4c6fe8446e48">

The most important term here is 1/L_t-1. Starting from x_0, as we conditionally unfold the equation, we can understand the normal distribution of the tractable forward process posterior q(x_t-1|x_t, x_0). Based on this, by calculating the KL divergence, we can ultimately train p_θ(x_t-1|x_t), which is what we aim to learn.

### Forward process and L_T

<img width="281" alt="Untitled 8" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/adfa8a3c-a542-42b4-8329-549047214a41">

*We ignore the fact that the forward process variances βt are learnable by reparameterization and instead fix them to constants. Thus, in our implementation, the approximate posterior q has no learnable parameters, so LT is a constant during training and can be ignored.*

### Reverse process and L_1:T-1

<img width="710" alt="Untitled 9" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/d46e9586-0cbd-4c4a-9547-e36671d95f25">

<img width="606" alt="Untitled 10" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/ec3fe377-f48e-41ab-a0bb-34a80f9dc13c">

<img width="618" alt="Untitled 11" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/d1577292-5c18-44ed-98cc-b79fdb6b4726">

<img width="628" alt="Untitled 12" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3af52879-fb30-432c-9ba7-5c0bcf8fd3c9">

- Train reverse process mean function approximator μθ to predict ˜μt or modify its parameterization for different predictions.
- Predicting x0 was tested but led to worse sample quality early in experiments.
- The chosen prediction parameterization resembles Langevin dynamics and simplifies the diffusion model’s variational bound into an objective resembling denoising score matching.
- This approach is another parameterization of pθ(xt−1|xt).
- Effectiveness verified in Section 4 through an ablation study comparing different prediction targets.

### Data scaling, reverse process decoder and L0

<img width="544" alt="Untitled 13" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/d0bd95be-f54e-487d-8e3d-79ce40d54924">

- Image data is scaled from integers {0,  ..., 255} to the range [−1, 1] for consistent neural network input scaling from the standard normal prior p(xT).
- Discrete log likelihoods are obtained through an independent discrete decoder based on a Gaussian distribution, leading to pθ(x0|x1) defined for discrete transitions.
- The setup allows for a variational bound that is a lossless code length of discrete data without adding noise or incorporating the Jacobian of the scaling operation.
- Future work may explore more powerful decoders like conditional autoregressive models.
- Upon sampling completion, µθ(x1,1) is displayed noiselessly.

### Simplified training objective

<img width="628" alt="Untitled 14" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/3c6b6d9d-9914-4c73-bf1a-ca6ab12d655d">

- Simplified objective discards weighting, resulting in a weighted variational bound.
- Focuses on different reconstruction aspects than the standard variational bound.
- Setup causes less emphasis on loss terms for small t values, prioritizing challenging denoising tasks at larger t values.
- Reweighting improves sample quality.

# Experiments

### Sample Quality

<img width="625" alt="Untitled 15" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/505ce5d7-b8d7-4120-8c51-955ff1f14abd">

### Reverse process parameterization and training objective ablation

- Predicting ˜µ is effective with the true variational bound, not as much with unweighted MSE.
- Learning reverse process variances results in unstable training and lower sample quality.
- Predicting with fixed variances and the simplified objective outperforms or matches ˜µ predictions on the variational bound.

### Progressive coding

**Progressive lossy compression**

<img width="603" alt="Untitled 16" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/4f44e864-ca61-417a-98be-f9fe303ff667">

<img width="618" alt="Untitled 17" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/8cfe4a72-2ac0-4ced-ba16-9209e91c7f3f">

**Progressive generation**

<img width="608" alt="Untitled 18" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/36f6d689-12fe-46c3-b86d-e2021326f55d">

**Connection to autoregressive decoding**

<img width="602" alt="Untitled 19" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/71195254-c0d8-4379-a1b1-46d71ac5ad10">

<img width="610" alt="Untitled 20" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/906a3f63-c025-4553-a694-4b56515ec381">

### Interpolation

<img width="615" alt="Untitled 21" src="https://github.com/jun-brro/deep-learning-paper-review/assets/115399447/2886daa4-0405-4138-ada8-2c4230226202">

# Conclusion

*We have presented high quality image samples using diffusion models, and we have found **connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics** (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.*
